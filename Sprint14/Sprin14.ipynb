{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[課題のURL](https://diver.diveintocode.jp/curriculums/1626)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprintディープラーニングフレームワーク2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】公式チュートリアルモデルを分担して実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KerasによるMLの基本,　保存と読み込み  \n",
    "→学習に時間がかかるので一度学習したデータを保存したい。主に下記の２点を紹介。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   \n",
    "\n",
    "下記のデータ保存をするコールバック関数をfit内の引数に設定する。   \n",
    "\n",
    "~~~\n",
    "    tf.keras.callbacks.ModelCheckpoint()\n",
    "~~~\n",
    "→エポックごとに保存するなど。  \n",
    "→1Epoch終了後に「保存されている重みの「Val_loss」＞学習後の「Val_loss」」の場合の時だけ保存するなど"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.   \n",
    "\n",
    "重みの値、モデルの設定、オプティマイザの設定ができる。  \n",
    "~~~\n",
    "    model.save(\"my_model.h5\")\n",
    "    models.load_model(\"my_model.h5\")\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
    "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 短いシーケンシャルモデルを返す関数\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "    keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "  \n",
    "    model.compile(optimizer='adam', \n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " 832/1000 [=======================>......] - ETA: 0s - loss: 1.2551 - acc: 0.6514\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 1s 809us/sample - loss: 1.1621 - acc: 0.6770 - val_loss: 0.7061 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      " 928/1000 [==========================>...] - ETA: 0s - loss: 0.4155 - acc: 0.8912\n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 291us/sample - loss: 0.4159 - acc: 0.8880 - val_loss: 0.5415 - val_acc: 0.8470\n",
      "Epoch 3/10\n",
      " 864/1000 [========================>.....] - ETA: 0s - loss: 0.2857 - acc: 0.9213\n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 362us/sample - loss: 0.2820 - acc: 0.9180 - val_loss: 0.4677 - val_acc: 0.8540\n",
      "Epoch 4/10\n",
      " 928/1000 [==========================>...] - ETA: 0s - loss: 0.2183 - acc: 0.9440\n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 1s 589us/sample - loss: 0.2173 - acc: 0.9460 - val_loss: 0.4428 - val_acc: 0.8550\n",
      "Epoch 5/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 0.1573 - acc: 0.9656\n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 356us/sample - loss: 0.1651 - acc: 0.9630 - val_loss: 0.4945 - val_acc: 0.8430\n",
      "Epoch 6/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 0.1242 - acc: 0.9750\n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 438us/sample - loss: 0.1229 - acc: 0.9750 - val_loss: 0.4303 - val_acc: 0.8610\n",
      "Epoch 7/10\n",
      " 832/1000 [=======================>......] - ETA: 0s - loss: 0.0896 - acc: 0.9880\n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 0.0916 - acc: 0.9860 - val_loss: 0.4065 - val_acc: 0.8660\n",
      "Epoch 8/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 0.0634 - acc: 0.9917\n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 358us/sample - loss: 0.0637 - acc: 0.9920 - val_loss: 0.4201 - val_acc: 0.8710\n",
      "Epoch 9/10\n",
      " 896/1000 [=========================>....] - ETA: 0s - loss: 0.0549 - acc: 0.9955\n",
      "Epoch 00009: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 429us/sample - loss: 0.0531 - acc: 0.9960 - val_loss: 0.4191 - val_acc: 0.8700\n",
      "Epoch 10/10\n",
      " 864/1000 [========================>.....] - ETA: 0s - loss: 0.0398 - acc: 0.9988\n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 0.0389 - acc: 0.9990 - val_loss: 0.4046 - val_acc: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1398cd198>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# チェックポイントコールバックを作る\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.fit(train_images, train_labels,  epochs = 10, \n",
    "          validation_data = (test_images,test_labels),\n",
    "          callbacks = [cp_callback])  # 訓練にコールバックを渡す\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                  cp.ckpt.data-00001-of-00002\n",
      "cp.ckpt.data-00000-of-00002 cp.ckpt.index\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 - 0s - loss: 2.3353 - acc: 0.1000\n",
      "Untrained model, accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = create_model()\n",
    "\n",
    "loss, acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(\"Untrained model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 - 0s - loss: 0.4046 - acc: 0.8720\n",
      "Restored model, accuracy: 87.20%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "loss,acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 287us/sample - loss: 1.1638 - acc: 0.6720\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 189us/sample - loss: 0.4191 - acc: 0.8870\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 201us/sample - loss: 0.2766 - acc: 0.9270\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 182us/sample - loss: 0.1847 - acc: 0.9590\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 188us/sample - loss: 0.1677 - acc: 0.9570\n",
      "WARNING:tensorflow:From /Users/mori/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/mori/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# モデル全体を１つのHDF5ファイルに保存します。\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# 重みとオプティマイザを含む全く同じモデルを再作成\n",
    "new_model = keras.models.load_model('my_model.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kerasへの書き換え"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KerasはTensorFLowに含まれるtf.kerasモジュールを使用してください。  \n",
    "\n",
    "KerasにはSequentialモデルかFunctional APIかなど書き方に種類がありますが、これは指定しません。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】Iris（2値分類）をKerasで学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64 samples, validate on 16 samples\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.7517 - accuracy: 0.5781 - val_loss: 0.5963 - val_accuracy: 0.5625\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 0s 633us/step - loss: 0.6747 - accuracy: 0.5781 - val_loss: 0.5246 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 0s 523us/step - loss: 0.5456 - accuracy: 0.7656 - val_loss: 0.4909 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 0s 643us/step - loss: 0.4984 - accuracy: 0.9219 - val_loss: 0.4739 - val_accuracy: 0.9375\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 0s 587us/step - loss: 0.4688 - accuracy: 0.9375 - val_loss: 0.4384 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 0s 445us/step - loss: 0.4414 - accuracy: 0.9219 - val_loss: 0.4008 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 0s 478us/step - loss: 0.4313 - accuracy: 0.9219 - val_loss: 0.4059 - val_accuracy: 0.9375\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 0s 442us/step - loss: 0.3955 - accuracy: 0.9375 - val_loss: 0.3564 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 0s 374us/step - loss: 0.3707 - accuracy: 0.9375 - val_loss: 0.3419 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 0s 399us/step - loss: 0.3492 - accuracy: 0.9531 - val_loss: 0.3214 - val_accuracy: 1.0000\n",
      "test: 0.9\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# モデルの作成\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "\n",
    "# モデルにレイヤーを積み上げていく\n",
    "model.add(Dense(n_hidden1, input_dim=n_input))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_hidden2, input_dim=n_hidden1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_classes))\n",
    "\n",
    "# 訓練プロセスの定義\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=Adam(lr = learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 訓練の実行\n",
    "# (x_train, y_trainはNumpy行列の学習データ)\n",
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# 予測の実行\n",
    "y_pred = model.predict_classes(X_test)\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(\"test:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                250       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 5,451\n",
      "Trainable params: 5,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】Iris（多値分類）をKerasで学習  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "def encode_3(data):\n",
    "    t = np.zeros((data.size, 3))\n",
    "    for i in range(data.size):\n",
    "        t[i, data[i]] = 1\n",
    "    return t\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "#df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")|(df[\"Species\"] == \"Iris-setosa\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y[y=='Iris-setosa'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "y = encode_3(y)\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/10\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 1.0383 - accuracy: 0.4688 - val_loss: 1.0014 - val_accuracy: 0.5417\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 0s 291us/step - loss: 0.7518 - accuracy: 0.7292 - val_loss: 0.9206 - val_accuracy: 0.5417\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 0s 261us/step - loss: 0.5665 - accuracy: 0.7396 - val_loss: 0.6741 - val_accuracy: 0.6250\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 0s 264us/step - loss: 0.4377 - accuracy: 0.8646 - val_loss: 0.6170 - val_accuracy: 0.5417\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 0s 260us/step - loss: 0.3960 - accuracy: 0.7500 - val_loss: 0.4345 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 0s 326us/step - loss: 0.3568 - accuracy: 0.8125 - val_loss: 0.5241 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 0s 283us/step - loss: 0.2945 - accuracy: 0.8958 - val_loss: 0.4775 - val_accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 0s 316us/step - loss: 0.2700 - accuracy: 0.8646 - val_loss: 0.3336 - val_accuracy: 0.8750\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 0s 263us/step - loss: 0.2313 - accuracy: 0.9375 - val_loss: 0.3783 - val_accuracy: 0.8750\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 0s 284us/step - loss: 0.1993 - accuracy: 0.9375 - val_loss: 0.3017 - val_accuracy: 0.8333\n",
      "test: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 5\n",
    "n_hidden2 = 10\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_samples_test = X_test.shape[0]\n",
    "# n_samples_val = X_val.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# モデルの作成\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "\n",
    "# モデルにレイヤーを積み上げていく\n",
    "model.add(Dense(n_hidden1, input_dim=n_input))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_hidden2, input_dim=n_hidden1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# 訓練プロセスの定義\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=Adam(lr = 0.02),\n",
    "              #optimizer=\"Adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 訓練の実行\n",
    "# (x_train, y_trainはNumpy行列の学習データ)\n",
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# 予測の実行\n",
    "y_pred = model.predict_classes(X_test)\n",
    "y_test_decode = np.argmax(y_test, axis=1)\n",
    "score = accuracy_score(y_pred, y_test_decode)\n",
    "print(\"test:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                60        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 33        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 118\n",
      "Trainable params: 118\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】House PricesをKerasで学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', index_col=0)\n",
    "\n",
    "# GrLivAreaとYearBuiltを抜き出す。\n",
    "X = df[[\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# 対数変換\n",
    "df[\"SalePrice\"] = np.log(df[\"SalePrice\"])\n",
    "df[\"GrLivArea\"] = np.log(df[\"GrLivArea\"])\n",
    "df[\"YearBuilt\"] = np.log(df[\"YearBuilt\"])\n",
    "\n",
    "# 外れ値の除去\n",
    "df.drop(df[(df[\"GrLivArea\"]>4500) & (df['SalePrice']<300000)].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = df[[\"GrLivArea\", \"YearBuilt\"]]\n",
    "#X = df[[\"GrLivArea\"]]\n",
    "y = df[\"SalePrice\"]\n",
    "# df = pd.concat([X, y], axis=1)\n",
    "#df.head(2)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 934 samples, validate on 234 samples\n",
      "Epoch 1/10\n",
      "934/934 [==============================] - 0s 224us/step - loss: 60.7484 - val_loss: 11.1883\n",
      "Epoch 2/10\n",
      "934/934 [==============================] - 0s 40us/step - loss: 3.2472 - val_loss: 5.4544\n",
      "Epoch 3/10\n",
      "934/934 [==============================] - 0s 35us/step - loss: 4.5648 - val_loss: 0.5116\n",
      "Epoch 4/10\n",
      "934/934 [==============================] - 0s 31us/step - loss: 0.4607 - val_loss: 0.9881\n",
      "Epoch 5/10\n",
      "934/934 [==============================] - 0s 38us/step - loss: 0.6258 - val_loss: 0.0574\n",
      "Epoch 6/10\n",
      "934/934 [==============================] - 0s 28us/step - loss: 0.1657 - val_loss: 0.1823\n",
      "Epoch 7/10\n",
      "934/934 [==============================] - 0s 31us/step - loss: 0.1096 - val_loss: 0.0703\n",
      "Epoch 8/10\n",
      "934/934 [==============================] - 0s 31us/step - loss: 0.1007 - val_loss: 0.0581\n",
      "Epoch 9/10\n",
      "934/934 [==============================] - 0s 30us/step - loss: 0.0778 - val_loss: 0.0623\n",
      "Epoch 10/10\n",
      "934/934 [==============================] - 0s 30us/step - loss: 0.0776 - val_loss: 0.0552\n",
      "test: 0.0708910725214825\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "# [X] \n",
    "# ハイパーパラメータの設定\n",
    "lr = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 10 # 利用しない\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# モデルの作成\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "\n",
    "# モデルにレイヤーを積み上げていく\n",
    "model.add(Dense(n_hidden1, input_dim=n_input))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_classes, input_dim=n_hidden1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 訓練プロセスの定義\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=Adam(lr = 0.01))\n",
    "              #optimizer=\"Adam\",\n",
    "              #metrics=['mean_squared_error'])\n",
    "\n",
    "# 訓練の実行\n",
    "# (x_train, y_trainはNumpy行列の学習データ)\n",
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# 予測の実行\n",
    "y_pred = model.predict(X_test)\n",
    "#socre = (np.square(y_pred - y_test)).mean(axis=1)\n",
    "score = mean_squared_error(y_pred, y_test)\n",
    "print(\"test:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】MNISTをKerasで学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# print(X_train.shape) # (60000, 28, 28)\n",
    "# print(X_test.shape) # (10000, 28, 28)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# print(X_train.max()) # 1.0\n",
    "# print(X_train.min()) # 0.0\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 65s 1ms/step - loss: 0.1929 - accuracy: 0.9416 - val_loss: 0.0698 - val_accuracy: 0.9787\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 50s 1ms/step - loss: 0.0646 - accuracy: 0.9803 - val_loss: 0.0690 - val_accuracy: 0.9777\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 63s 1ms/step - loss: 0.0453 - accuracy: 0.9861 - val_loss: 0.0672 - val_accuracy: 0.9803\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.0348 - accuracy: 0.9889 - val_loss: 0.0506 - val_accuracy: 0.9860\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 48s 996us/step - loss: 0.0266 - accuracy: 0.9913 - val_loss: 0.0592 - val_accuracy: 0.9835\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 43s 894us/step - loss: 0.0218 - accuracy: 0.9928 - val_loss: 0.0525 - val_accuracy: 0.9853\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 60s 1ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.0593 - val_accuracy: 0.9851\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 56s 1ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 0.0585 - val_accuracy: 0.9847\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 57s 1ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0642 - val_accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 0.0091 - accuracy: 0.9967 - val_loss: 0.0661 - val_accuracy: 0.9858\n",
      "test: 0.9874\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "def _encode_10(data):\n",
    "    t = np.zeros((data.size, 10))\n",
    "    for i in range(data.size):\n",
    "        t[i, data[i]] = 1\n",
    "    return t\n",
    "\n",
    "\"\"\"\n",
    "Kerasで実装したニューラルネットワークを使いMINSTを分類する\n",
    "\"\"\"\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 20\n",
    "num_epochs = 10\n",
    "n_hidden1 = 40\n",
    "n_hidden2 = 20\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_samples_test = X_test.shape[0]\n",
    "n_samples_val = X_val.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "filter_num = 30\n",
    "filter_size = 5\n",
    "filter_pad = 0\n",
    "filter_stride = 1\n",
    "hidden_size = 100\n",
    "input_size = 28\n",
    "output_size = 10\n",
    "max_pool_size1 = 2\n",
    "\n",
    "# モデルの作成\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(n_hidden1, (filter_size, filter_size), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(max_pool_size1, max_pool_size1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_hidden2, activation='relu'))\n",
    "model.add(Dense(output_size, activation='softmax'))\n",
    "\n",
    "# 訓練プロセスの定義\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr = 0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# データの型を合わせる\n",
    "X_train_4d = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "y_train_one_hot = _encode_10(y_train)\n",
    "\n",
    "X_test_4d = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "y_test_one_hot = _encode_10(y_test)\n",
    "\n",
    "model.fit(X_train_4d, y_train_one_hot, epochs=num_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# 予測の実行\n",
    "X_test_4d = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "y_test_one_hot = _encode_10(y_test)\n",
    "\n",
    "y_pred = model.predict_classes(X_test_4d)\n",
    "\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(\"test:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
